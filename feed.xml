<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Awesome Gaussian Splatting Latest Papers</title><link>https://yourusername.github.io/Awesome-Gaussian-Splatting</link><description>Daily updated feed of the latest Gaussian Splatting papers from arXiv.</description><language>en-us</language><lastBuildDate>Sun, 22 Feb 2026 06:00:28 +0000</lastBuildDate><atom:link href="https://yourusername.github.io/Awesome-Gaussian-Splatting/feed.xml" rel="self" type="application/rss+xml" /><item><title>4D Monocular Surgical Reconstruction under Arbitrary Camera Motions</title><link>https://arxiv.org/abs/2602.17473</link><guid>https://arxiv.org/abs/2602.17473</guid><description>Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in ...</description><pubDate>Thu, 19 Feb 2026 15:37:27 +0000</pubDate><category>Dynamic</category><category>Medical</category><category>Physics</category><author>Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Yirui Li, Hao Liu</author><author>Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Yirui Li, Hao Liu et al.</author></item><item><title>NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.17182</link><guid>https://arxiv.org/abs/2602.17182</guid><description>Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representation...</description><pubDate>Thu, 19 Feb 2026 09:03:47 +0000</pubDate><category>Dynamic</category><category>Medical</category><category>Physics</category><category>Robotics</category><category>SLAM</category><author>Jiwei Shan, Zeyu Cai, Yirui Li, Yongbo Chen, Lijun Han</author><author>Jiwei Shan, Zeyu Cai, Yirui Li, Yongbo Chen, Lijun Han et al.</author></item><item><title>B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates</title><link>https://arxiv.org/abs/2602.17134</link><guid>https://arxiv.org/abs/2602.17134</guid><description>Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Ou...</description><pubDate>Thu, 19 Feb 2026 07:14:52 +0000</pubDate><category>Editing</category><category>Language</category><category>Segmentation</category><author>Hiromichi Kamata, Samuel Arthur Munro, Fuminori Homma</author></item><item><title>3D Scene Rendering with Multimodal Gaussian Splatting</title><link>https://arxiv.org/abs/2602.17124</link><guid>https://arxiv.org/abs/2602.17124</guid><description>3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typic...</description><pubDate>Thu, 19 Feb 2026 06:49:53 +0000</pubDate><category>Autonomous Driving</category><category>Robotics</category><author>Chi-Shiang Gau, Konstantinos D. Polyzos, Athanasios Bacharis, Saketh Madhuvarasu, Tara Javidi</author></item><item><title>i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.17117</link><guid>https://arxiv.org/abs/2602.17117</guid><description>Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework ...</description><pubDate>Thu, 19 Feb 2026 06:38:35 +0000</pubDate><category>Dynamic</category><category>Physics</category><author>Yicheng Cao, Zhuo Huang, Yu Yao, Yiming Ying, Daoyi Dong</author><author>Yicheng Cao, Zhuo Huang, Yu Yao, Yiming Ying, Daoyi Dong et al.</author></item><item><title>Semantic-Guided 3D Gaussian Splatting for Transient Object Removal</title><link>https://arxiv.org/abs/2602.15516</link><guid>https://arxiv.org/abs/2602.15516</guid><description>Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across trai...</description><pubDate>Tue, 17 Feb 2026 11:44:16 +0000</pubDate><category>Compression</category><category>Dynamic</category><category>Language</category><category>Rendering</category><category>Segmentation</category><author>Aditi Prabakaran, Priyesh Shukla</author></item><item><title>DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles</title><link>https://arxiv.org/abs/2602.15355</link><guid>https://arxiv.org/abs/2602.15355</guid><description>The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and acti...</description><pubDate>Tue, 17 Feb 2026 04:47:39 +0000</pubDate><category>Generation</category><author>Rong Fu, Jiekai Wu, Haiyun Wei, Yee Tan Jia, Wenxin Zhang</author><author>Rong Fu, Jiekai Wu, Haiyun Wei, Yee Tan Jia, Wenxin Zhang et al.</author></item><item><title>Time-Archival Camera Virtualization for Sports and Visual Performances</title><link>https://arxiv.org/abs/2602.15181</link><guid>https://arxiv.org/abs/2602.15181</guid><description>Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularl...</description><pubDate>Mon, 16 Feb 2026 20:39:51 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Physics</category><category>Rendering</category><category>Segmentation</category><author>Yunxiao Zhang, William Stone, Suryansh Kumar</author></item><item><title>Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery</title><link>https://arxiv.org/abs/2602.14929</link><guid>https://arxiv.org/abs/2602.14929</guid><description>Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth...</description><pubDate>Mon, 16 Feb 2026 17:06:54 +0000</pubDate><category>Robotics</category><category>Segmentation</category><author>Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati</author><author>Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati et al.</author></item><item><title>Gaussian Mesh Renderer for Lightweight Differentiable Rendering</title><link>https://arxiv.org/abs/2602.14493</link><guid>https://arxiv.org/abs/2602.14493</guid><description>3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR...</description><pubDate>Mon, 16 Feb 2026 06:15:42 +0000</pubDate><category>Avatar</category><category>Compression</category><category>Mesh</category><category>Rendering</category><author>Xinpeng Liu, Fumio Okura</author></item><item><title>Learnable Multi-level Discrete Wavelet Transforms for 3D Gaussian Splatting Frequency Modulation</title><link>https://arxiv.org/abs/2602.14199</link><guid>https://arxiv.org/abs/2602.14199</guid><description>3D Gaussian Splatting (3DGS) has emerged as a powerful approach for novel view synthesis. However, the number of Gaussian primitives often grows substantially during training as finer scene details are reconstructed, leading to increased memory and storage costs. Recent coarse-to-fine strategies regulate Gaussian growth by modulating the frequency content of the ground-truth images. In particular, AutoOpti3DGS employs the learnable Discrete Wavelet Transform (DWT) to enable data-adaptive freq...</description><pubDate>Sun, 15 Feb 2026 15:49:03 +0000</pubDate><category>Rendering</category><author>Hung Nguyen, An Le, Truong Nguyen</author></item><item><title>High-fidelity 3D reconstruction for planetary exploration</title><link>https://arxiv.org/abs/2602.13909</link><guid>https://arxiv.org/abs/2602.13909</guid><description>Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Stru...</description><pubDate>Sat, 14 Feb 2026 22:07:03 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Robotics</category><category>SLAM</category><author>Alfonso Martínez-Petersen, Levin Gerdes, David Rodríguez-Martínez, C. J. Pérez-del-Pulgar</author></item><item><title>Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos</title><link>https://arxiv.org/abs/2602.13806</link><guid>https://arxiv.org/abs/2602.13806</guid><description>Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dy...</description><pubDate>Sat, 14 Feb 2026 14:30:25 +0000</pubDate><category>Dynamic</category><category>Editing</category><category>Physics</category><category>Rendering</category><category>Robotics</category><author>Can Li, Jie Gu, Jingmin Chen, Fangzhou Qiu, Lei Sun</author></item><item><title>Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields</title><link>https://arxiv.org/abs/2602.13801</link><guid>https://arxiv.org/abs/2602.13801</guid><description>We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additiona...</description><pubDate>Sat, 14 Feb 2026 14:27:07 +0000</pubDate><category>Avatar</category><category>Mesh</category><author>Jiaze Li, Daisheng Jin, Fei Hou, Junhui Hou, Zheng Liu</author><author>Jiaze Li, Daisheng Jin, Fei Hou, Junhui Hou, Zheng Liu et al.</author></item><item><title>Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting</title><link>https://arxiv.org/abs/2602.13549</link><guid>https://arxiv.org/abs/2602.13549</guid><description>This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing met...</description><pubDate>Sat, 14 Feb 2026 01:49:23 +0000</pubDate><category>Autonomous Driving</category><category>Physics</category><category>Rendering</category><author>Tae-Kyeong Kim, Xingxin Chen, Guile Wu, Chengjie Huang, Dongfeng Bai</author><author>Tae-Kyeong Kim, Xingxin Chen, Guile Wu, Chengjie Huang, Dongfeng Bai et al.</author></item><item><title>FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation</title><link>https://arxiv.org/abs/2602.13444</link><guid>https://arxiv.org/abs/2602.13444</guid><description>Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, tempo...</description><pubDate>Fri, 13 Feb 2026 20:46:08 +0000</pubDate><category>Compression</category><category>Dynamic</category><category>Editing</category><category>Generation</category><category>Language</category><category>Physics</category><category>Robotics</category><category>Segmentation</category><author>Huajian Zeng, Lingyun Chen, Jiaqi Yang, Yuantai Zhang, Fan Shi</author><author>Huajian Zeng, Lingyun Chen, Jiaqi Yang, Yuantai Zhang, Fan Shi et al.</author></item><item><title>GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction</title><link>https://arxiv.org/abs/2602.12796</link><guid>https://arxiv.org/abs/2602.12796</guid><description>Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framewor...</description><pubDate>Fri, 13 Feb 2026 10:26:32 +0000</pubDate><category>Avatar</category><category>Dynamic</category><category>Mesh</category><author>Xiao Ren, Yu Liu, Ning An, Jian Cheng, Xin Qiao</author><author>Xiao Ren, Yu Liu, Ning An, Jian Cheng, Xin Qiao et al.</author></item><item><title>LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning</title><link>https://arxiv.org/abs/2602.12314</link><guid>https://arxiv.org/abs/2602.12314</guid><description>We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our a...</description><pubDate>Thu, 12 Feb 2026 17:25:00 +0000</pubDate><category>Compression</category><category>Language</category><category>Robotics</category><category>Segmentation</category><author>Junwoon Lee, Yulun Tian</author></item><item><title>3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.12159</link><guid>https://arxiv.org/abs/2602.12159</guid><description>Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a ...</description><pubDate>Thu, 12 Feb 2026 16:41:26 +0000</pubDate><category>Language</category><category>Robotics</category><category>Segmentation</category><author>Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</author></item><item><title>GSO-SLAM: Bidirectionally Coupled Gaussian Splatting and Direct Visual Odometry</title><link>https://arxiv.org/abs/2602.11714</link><guid>https://arxiv.org/abs/2602.11714</guid><description>We propose GSO-SLAM, a real-time monocular dense SLAM system that leverages Gaussian scene representation. Unlike existing methods that couple tracking and mapping with a unified scene, incurring computational costs, or loosely integrate them with well-structured tracking frameworks, introducing redundancies, our method bidirectionally couples Visual Odometry (VO) and Gaussian Splatting (GS). Specifically, our approach formulates joint optimization within an Expectation-Maximization (EM) fram...</description><pubDate>Thu, 12 Feb 2026 08:44:32 +0000</pubDate><category>SLAM</category><author>Jiung Yeon, Seongbo Ha, Hyeonwoo Yu</author></item><item><title>TG-Field: Geometry-Aware Radiative Gaussian Fields for Tomographic Reconstruction</title><link>https://arxiv.org/abs/2602.11705</link><guid>https://arxiv.org/abs/2602.11705</guid><description>3D Gaussian Splatting (3DGS) has revolutionized 3D scene representation with superior efficiency and quality. While recent adaptations for computed tomography (CT) show promise, they struggle with severe artifacts under highly sparse-view projections and dynamic motions. To address these challenges, we propose Tomographic Geometry Field (TG-Field), a geometry-aware Gaussian deformation framework tailored for both static and dynamic CT reconstruction. A multi-resolution hash encoder is employe...</description><pubDate>Thu, 12 Feb 2026 08:33:01 +0000</pubDate><category>Dynamic</category><category>Medical</category><category>Physics</category><category>Sparse View</category><author>Yuxiang Zhong, Jun Wei, Chaoqi Chen, Senyou An, Hui Huang</author></item><item><title>OMEGA-Avatar: One-shot Modeling of 360° Gaussian Avatars</title><link>https://arxiv.org/abs/2602.11693</link><guid>https://arxiv.org/abs/2602.11693</guid><description>Creating high-fidelity, animatable 3D avatars from a single image remains a formidable challenge. We identified three desirable attributes of avatar generation: 1) the method should be feed-forward, 2) model a 360° full-head, and 3) should be animation-ready. However, current work addresses only two of the three points simultaneously. To address these limitations, we propose OMEGA-Avatar, the first feed-forward framework that simultaneously generates a generalizable, 360°-complete, and animat...</description><pubDate>Thu, 12 Feb 2026 08:16:38 +0000</pubDate><category>Avatar</category><category>Generation</category><category>Mesh</category><category>Physics</category><category>Segmentation</category><category>Sparse View</category><author>Zehao Xia, Yiqun Wang, Zhengda Lu, Kai Liu, Jun Xiao</author><author>Zehao Xia, Yiqun Wang, Zhengda Lu, Kai Liu, Jun Xiao et al.</author></item><item><title>GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction</title><link>https://arxiv.org/abs/2602.11653</link><guid>https://arxiv.org/abs/2602.11653</guid><description>Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision. In this work, we pro-pose a novel GR-Diffusion framework that synerg...</description><pubDate>Thu, 12 Feb 2026 07:10:38 +0000</pubDate><category>Generation</category><category>Medical</category><category>Physics</category><author>Mengxiao Geng, Zijie Chen, Ran Hong, Bingxuan Li, Qiegen Liu</author></item><item><title>Variation-aware Flexible 3D Gaussian Editing</title><link>https://arxiv.org/abs/2602.11638</link><guid>https://arxiv.org/abs/2602.11638</guid><description>Indirect editing methods for 3D Gaussian Splatting (3DGS) have recently witnessed significant advancements. These approaches operate by first applying edits in the rendered 2D space and subsequently projecting the modifications back into 3D. However, this paradigm inevitably introduces cross-view inconsistencies and constrains both the flexibility and efficiency of the editing process. To address these challenges, we present VF-Editor, which enables native editing of Gaussian primitives by pr...</description><pubDate>Thu, 12 Feb 2026 06:43:04 +0000</pubDate><category>Editing</category><category>Generation</category><author>Hao Qin, Yukai Sun, Meng Wang, Ming Kong, Mengxu Lu</author><author>Hao Qin, Yukai Sun, Meng Wang, Ming Kong, Mengxu Lu et al.</author></item><item><title>LeafFit: Plant Assets Creation from 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.11577</link><guid>https://arxiv.org/abs/2602.11577</guid><description>We propose LeafFit, a pipeline that converts 3D Gaussian Splatting (3DGS) of individual plants into editable, instanced mesh assets. While 3DGS faithfully captures complex foliage, its high memory footprint and lack of mesh topology make it incompatible with traditional game production workflows. We address this by leveraging the repetition of leaf shapes; our method segments leaves from the unstructured 3DGS, with optional user interaction included as a fallback. A representative leaf group ...</description><pubDate>Thu, 12 Feb 2026 04:54:41 +0000</pubDate><category>Editing</category><category>Mesh</category><category>Physics</category><category>Segmentation</category><author>Chang Luo, Nobuyuki Umetani</author></item><item><title>ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles</title><link>https://arxiv.org/abs/2602.11575</link><guid>https://arxiv.org/abs/2602.11575</guid><description>Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation using 3D Gaussian Splatting (GS) can mitigate these challenges, prior GS-based works have considered only static scenes or non-photorealistic human obstacles built from simulator assets, despite the ...</description><pubDate>Thu, 12 Feb 2026 04:48:18 +0000</pubDate><category>Avatar</category><category>Dynamic</category><category>Generation</category><category>Physics</category><category>Robotics</category><author>Seungyeon Yoo, Youngseok Jang, Dabin Kim, Youngsoo Han, Seungwoo Jung</author><author>Seungyeon Yoo, Youngseok Jang, Dabin Kim, Youngsoo Han, Seungwoo Jung et al.</author></item><item><title>SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos</title><link>https://arxiv.org/abs/2602.11154</link><guid>https://arxiv.org/abs/2602.11154</guid><description>Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussia...</description><pubDate>Wed, 11 Feb 2026 18:59:55 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Mesh</category><category>Rendering</category><author>Yue Gao, Hong-Xing Yu, Sanghyeon Chang, Qianxi Fu, Bo Zhu</author><author>Yue Gao, Hong-Xing Yu, Sanghyeon Chang, Qianxi Fu, Bo Zhu et al.</author></item><item><title>ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.10278</link><guid>https://arxiv.org/abs/2602.10278</guid><description>Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signa...</description><pubDate>Tue, 10 Feb 2026 20:44:43 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Sparse View</category><author>Zehua Ma, Hanhui Li, Zhenyu Xie, Xiaonan Luo, Michael Kampffmeyer</author><author>Zehua Ma, Hanhui Li, Zhenyu Xie, Xiaonan Luo, Michael Kampffmeyer et al.</author></item><item><title>XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability</title><link>https://arxiv.org/abs/2602.10239</link><guid>https://arxiv.org/abs/2602.10239</guid><description>3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, pr...</description><pubDate>Tue, 10 Feb 2026 19:35:42 +0000</pubDate><category>Generation</category><author>Dominik Galus, Julia Farganus, Tymoteusz Zapala, Mikołaj Czachorowski, Piotr Borycki</author><author>Dominik Galus, Julia Farganus, Tymoteusz Zapala, Mikołaj Czachorowski, Piotr Borycki et al.</author></item><item><title>Faster-GS: Analyzing and Improving Gaussian Splatting Optimization</title><link>https://arxiv.org/abs/2602.09999</link><guid>https://arxiv.org/abs/2602.09999</guid><description>Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them ...</description><pubDate>Tue, 10 Feb 2026 17:22:59 +0000</pubDate><category>Dynamic</category><author>Florian Hahlbohm, Linus Franke, Martin Eisemann, Marcus Magnor</author></item><item><title>ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop</title><link>https://arxiv.org/abs/2602.10173</link><guid>https://arxiv.org/abs/2602.10173</guid><description>Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduc...</description><pubDate>Tue, 10 Feb 2026 16:00:03 +0000</pubDate><category>Editing</category><category>Generation</category><category>Physics</category><category>Segmentation</category><author>Clement Fuji Tsang, Anita Hu, Or Perel, Carsten Kolve, Maria Shugrina</author></item><item><title>CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video</title><link>https://arxiv.org/abs/2602.09816</link><guid>https://arxiv.org/abs/2602.09816</guid><description>High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rende...</description><pubDate>Tue, 10 Feb 2026 14:23:42 +0000</pubDate><category>Compression</category><category>Rendering</category><author>Hojun Song, Heejung Choi, Aro Kim, Chae-yeong Song, Gahyeon Kim</author><author>Hojun Song, Heejung Choi, Aro Kim, Chae-yeong Song, Gahyeon Kim et al.</author></item><item><title>Toward Fine-Grained Facial Control in 3D Talking Head Generation</title><link>https://arxiv.org/abs/2602.09736</link><guid>https://arxiv.org/abs/2602.09736</guid><description>Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (...</description><pubDate>Tue, 10 Feb 2026 12:49:50 +0000</pubDate><category>Avatar</category><category>Dynamic</category><category>Generation</category><category>Rendering</category><author>Shaoyang Xie, Xiaofeng Cong, Baosheng Yu, Zhipeng Gui, Jie Gui</author><author>Shaoyang Xie, Xiaofeng Cong, Baosheng Yu, Zhipeng Gui, Jie Gui et al.</author></item><item><title>Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting</title><link>https://arxiv.org/abs/2602.09415</link><guid>https://arxiv.org/abs/2602.09415</guid><description>We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are...</description><pubDate>Tue, 10 Feb 2026 05:11:06 +0000</pubDate><author>Joe-Mei Feng, Hsin-Hsiung Kao</author></item><item><title>Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields</title><link>https://arxiv.org/abs/2602.08958</link><guid>https://arxiv.org/abs/2602.08958</guid><description>Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introdu...</description><pubDate>Mon, 09 Feb 2026 17:55:01 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Physics</category><author>Weihan Luo, Lily Goli, Sherwin Bahmani, Felix Taubner, Andrea Tagliasacchi</author><author>Weihan Luo, Lily Goli, Sherwin Bahmani, Felix Taubner, Andrea Tagliasacchi et al.</author></item><item><title>Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit</title><link>https://arxiv.org/abs/2602.08909</link><guid>https://arxiv.org/abs/2602.08909</guid><description>We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncover...</description><pubDate>Mon, 09 Feb 2026 17:09:08 +0000</pubDate><category>Rendering</category><author>Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang</author><author>Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang et al.</author></item><item><title>GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion</title><link>https://arxiv.org/abs/2602.08784</link><guid>https://arxiv.org/abs/2602.08784</guid><description>Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping ...</description><pubDate>Mon, 09 Feb 2026 15:25:19 +0000</pubDate><category>Dynamic</category><category>Robotics</category><category>Segmentation</category><author>Santiago Montiel-Marín, Miguel Antunes-García, Fabio Sánchez-García, Angel Llamazares, Holger Caesar</author><author>Santiago Montiel-Marín, Miguel Antunes-García, Fabio Sánchez-García, Angel Llamazares, Holger Caesar et al.</author></item><item><title>Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering</title><link>https://arxiv.org/abs/2602.08724</link><guid>https://arxiv.org/abs/2602.08724</guid><description>Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting th...</description><pubDate>Mon, 09 Feb 2026 14:34:06 +0000</pubDate><category>Editing</category><category>Mesh</category><category>Rendering</category><author>Geng Lin, Matthias Zwicker</author></item><item><title>FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction</title><link>https://arxiv.org/abs/2602.08558</link><guid>https://arxiv.org/abs/2602.08558</guid><description>We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that...</description><pubDate>Mon, 09 Feb 2026 11:55:15 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Physics</category><category>Rendering</category><author>Guan Yuan Tan, Ngoc Tuan Vu, Arghya Pal, Sailaja Rajanala, Raphael Phan C. -W.</author><author>Guan Yuan Tan, Ngoc Tuan Vu, Arghya Pal, Sailaja Rajanala, Raphael Phan C. -W. et al.</author></item><item><title>Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes</title><link>https://arxiv.org/abs/2602.08266</link><guid>https://arxiv.org/abs/2602.08266</guid><description>In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation...</description><pubDate>Mon, 09 Feb 2026 04:50:36 +0000</pubDate><category>Editing</category><category>Robotics</category><category>Segmentation</category><author>Seunghoon Jeong, Eunho Lee, Jeongyun Kim, Ayoung Kim</author></item><item><title>Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video</title><link>https://arxiv.org/abs/2602.07891</link><guid>https://arxiv.org/abs/2602.07891</guid><description>Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video str...</description><pubDate>Sun, 08 Feb 2026 09:53:21 +0000</pubDate><author>Zihui Gao, Ke Liu, Donny Y. Chen, Duochao Shi, Guosheng Lin</author><author>Zihui Gao, Ke Liu, Donny Y. Chen, Duochao Shi, Guosheng Lin et al.</author></item><item><title>Thermal odometry and dense mapping using learned odometry and Gaussian splatting</title><link>https://arxiv.org/abs/2602.07493</link><guid>https://arxiv.org/abs/2602.07493</guid><description>Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high...</description><pubDate>Sat, 07 Feb 2026 11:07:44 +0000</pubDate><category>Dynamic</category><category>Robotics</category><category>SLAM</category><author>Tianhao Zhou, Yujia Chen, Zhihao Zhan, Yuhang Ming, Jianzhu Huai</author></item><item><title>DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos</title><link>https://arxiv.org/abs/2602.06846</link><guid>https://arxiv.org/abs/2602.06846</guid><description>Spatial audio is crucial for creating compelling immersive 360-degree video experiences. However, generating realistic spatial audio, such as first-order ambisonics (FOA), from 360-degree videos in complex acoustic scenes remains challenging. Existing methods often overlook the dynamic nature and acoustic complexity of 360-degree scenes, fail to fully account for dynamic sound sources, and neglect complex environmental effects such as occlusion, reflections, and reverberation, which are influ...</description><pubDate>Fri, 06 Feb 2026 16:36:46 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Segmentation</category><author>Ziyu Luo, Lin Chen, Qiang Qu, Xiaoming Chen, Yiran Shen</author></item><item><title>GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification</title><link>https://arxiv.org/abs/2602.06830</link><guid>https://arxiv.org/abs/2602.06830</guid><description>Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS...</description><pubDate>Fri, 06 Feb 2026 16:17:41 +0000</pubDate><category>Compression</category><author>Soonbin Lee, Yeong-Gyu Kim, Simon Sasse, Tomas M. Borges, Yago Sanchez</author><author>Soonbin Lee, Yeong-Gyu Kim, Simon Sasse, Tomas M. Borges, Yago Sanchez et al.</author></item><item><title>Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2602.07101</link><guid>https://arxiv.org/abs/2602.07101</guid><description>UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effect...</description><pubDate>Fri, 06 Feb 2026 15:51:03 +0000</pubDate><category>Compression</category><category>Dynamic</category><category>Editing</category><category>Physics</category><category>Robotics</category><author>Zinan Lv, Yeqian Qian, Chen Sang, Hao Liu, Danping Zou</author><author>Zinan Lv, Yeqian Qian, Chen Sang, Hao Liu, Danping Zou et al.</author></item><item><title>TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction</title><link>https://arxiv.org/abs/2602.06400</link><guid>https://arxiv.org/abs/2602.06400</guid><description>3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely h...</description><pubDate>Fri, 06 Feb 2026 05:43:42 +0000</pubDate><category>Autonomous Driving</category><category>Dynamic</category><category>Robotics</category><category>Segmentation</category><author>Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall</author></item><item><title>Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering</title><link>https://arxiv.org/abs/2602.06343</link><guid>https://arxiv.org/abs/2602.06343</guid><description>High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose ...</description><pubDate>Fri, 06 Feb 2026 03:14:37 +0000</pubDate><category>Dynamic</category><category>Generation</category><category>Physics</category><author>Weiquan Wang, Feifei Shao, Lin Li, Zhen Wang, Jun Xiao</author><author>Weiquan Wang, Feifei Shao, Lin Li, Zhen Wang, Jun Xiao et al.</author></item><item><title>From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors</title><link>https://arxiv.org/abs/2602.06122</link><guid>https://arxiv.org/abs/2602.06122</guid><description>Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Desp...</description><pubDate>Thu, 05 Feb 2026 19:00:50 +0000</pubDate><category>Avatar</category><category>Dynamic</category><category>Generation</category><author>Ding-Jiun Huang, Yuanhao Wang, Shao-Ji Yuan, Albert Mosella-Montoro, Francisco Vicente Carrasco</author><author>Ding-Jiun Huang, Yuanhao Wang, Shao-Ji Yuan, Albert Mosella-Montoro, Francisco Vicente Carrasco et al.</author></item><item><title>Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</title><link>https://arxiv.org/abs/2602.06032</link><guid>https://arxiv.org/abs/2602.06032</guid><description>Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian re...</description><pubDate>Thu, 05 Feb 2026 18:59:05 +0000</pubDate><category>Dynamic</category><category>Rendering</category><category>Segmentation</category><author>David Shavin, Sagie Benaim</author></item><item><title>NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects</title><link>https://arxiv.org/abs/2602.05822</link><guid>https://arxiv.org/abs/2602.05822</guid><description>We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an...</description><pubDate>Thu, 05 Feb 2026 16:13:53 +0000</pubDate><category>Rendering</category><category>Robotics</category><author>Musawar Ali, Manuel Carranza-García, Nicola Fioraio, Samuele Salti, Luigi Di Stefano</author></item></channel></rss>